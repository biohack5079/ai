import os
from fastapi import FastAPI, Request, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import requests
from google import genai
from dotenv import load_dotenv

# .envãƒ­ãƒ¼ãƒ‰
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
client = None
AVAILABLE_GEMINI_MODELS = [] # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿æŒ

app = FastAPI()

# --- ğŸš€ èµ·å‹•æ™‚ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„ã«å–å¾— ---
@app.on_event("startup")
async def startup_event():
    global client, AVAILABLE_GEMINI_MODELS
    if GEMINI_API_KEY:
        try:
            client = genai.Client(api_key=GEMINI_API_KEY)
            # models.list() ã§åˆ©ç”¨å¯èƒ½ãªå…¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            # names ã¯ "models/gemini-2.5-flash" ã®å½¢å¼ãªã®ã§ "models/" ã‚’é™¤å»
            models = client.models.list()
            AVAILABLE_GEMINI_MODELS = [m.name.replace("models/", "") for m in models]
            print(f"âœ… Loaded Gemini models: {AVAILABLE_GEMINI_MODELS}")
        except Exception as e:
            print(f"âŒ Failed to load Gemini models: {e}")

# --- ğŸ§  ãƒ¢ãƒ‡ãƒ«åã®ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒãƒƒãƒ”ãƒ³ã‚°é–¢æ•° ---
def map_model_name(user_model: str) -> str:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: 'flash', 'gemini-1.5-pro'ï¼‰ã‚’
    ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªæœ€æ–°ã®æ­£å¼ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›ã™ã‚‹
    """
    if not AVAILABLE_GEMINI_MODELS:
        return user_model # ãƒªã‚¹ãƒˆãŒç©ºãªã‚‰ãã®ã¾ã¾è¿”ã™

    # 1. å®Œå…¨ä¸€è‡´ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
    if user_model in AVAILABLE_GEMINI_MODELS:
        return user_model

    # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆflash, proï¼‰ãŒå«ã¾ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    # ä¾‹: "gemini-flash" -> "flash" ã§æ¤œç´¢
    search_keyword = user_model.replace("gemini-", "").replace("1.5-", "").replace("2.5-", "")
    
    candidates = [
        m for m in AVAILABLE_GEMINI_MODELS 
        if search_keyword in m and "vision" not in m # visionå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ç­‰ã¯é™¤å¤–
    ]

    if candidates:
        # æ–‡å­—åˆ—é †ã§ã‚½ãƒ¼ãƒˆã—ã¦æœ€æ–°ï¼ˆä¾‹: 2.5 > 1.5ï¼‰ã‚’é¸æŠ
        return sorted(candidates)[-1]

    # 3. è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ï¼ˆOllamaç”¨ãªã©ï¼‰å…¥åŠ›ã‚’ãã®ã¾ã¾è¿”ã™
    return user_model

# --- CORSè¨­å®š ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class GeminiRequest(BaseModel):
    model: str = "gemini-flash" # ãƒ•ãƒ­ãƒ³ãƒˆã‹ã‚‰ã®æŠ½è±¡çš„ãªæŒ‡å®š
    prompt: str
    temperature: float = 0.1
    
@app.get("/")
def read_root():
    return {"status": "online", "available_models_count": len(AVAILABLE_GEMINI_MODELS)}

# Gemini APIã‚’ä¸­ç¶™ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚·ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/api/gemini_proxy")
async def gemini_proxy(request_data: GeminiRequest):
    if not client:
        raise HTTPException(status_code=503, detail="Gemini Client not initialized.")

    # âœ¨ ã“ã“ã§å‹•çš„ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨
    actual_model = map_model_name(request_data.model)
    print(f"ğŸ”€ Mapping: {request_data.model} -> {actual_model}")

    try:
        response = client.models.generate_content(
            model=actual_model,
            contents=request_data.prompt,
            config=genai.types.GenerateContentConfig(
                temperature=request_data.temperature
            )
        )
        return {"response": response.text, "model_used": actual_model}
        
    except Exception as e:
        print(f"Gemini API Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Sarasina (OllamaçµŒç”±ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼) ç”¨ã®ãƒ—ãƒ­ã‚­ã‚·ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/api/sarasina")
async def sarasina_proxy(request_data: GeminiRequest):
    # (Sarasinaã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—ã§OK)
    target_url = "http://localhost:11434/api/chat"
    try:
        response = requests.post(
            target_url,
            json={
                "model": request_data.model,
                "messages": [{"role": "user", "content": request_data.prompt}],
                "stream": False,
                "options": {"temperature": request_data.temperature, "num_ctx": 8192}
            }
        )
        response.raise_for_status()
        return {"response": response.json()["message"]["content"]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ollama Error: {str(e)}")